{
 "cells": [
  {
   "cell_type": "raw",
   "source": [
    "Verifica la correttezza dell'output della classe Layer confrontando i risultati con quelli di pytorch."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('../') #serve per fare l'import di file in un'altra directory sullo stesso \"livello\"\n",
    "from src.neuralNetwork.Layer import Layer\n",
    "from src.neuralNetwork.function import ReLuFunction, SigmoidFunction, LeakReLuFunction, TanhFunction, SoftplusFunction, IdentityFunction\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test_layer(num_neurons, num_inputs, activationTorch, activationPersonale):\n",
    "    target_inputs = np.matrix(np.random.rand(10000, num_inputs))\n",
    "\n",
    "    #Definzione layer pytorch\n",
    "    layerTorch = nn.Linear(num_inputs, num_neurons, bias=True, dtype=torch.float64)\n",
    "\n",
    "    outputTorch = layerTorch.forward(torch.tensor(target_inputs, dtype=torch.float64))\n",
    "    outputTorch = activationTorch(outputTorch)\n",
    "\n",
    "    #definizione layer personale\n",
    "    layer = Layer(num_neurons, num_inputs, activation_function=activationPersonale)\n",
    "    layer.weights = layerTorch.weight.detach().numpy()\n",
    "    layer.biases = layerTorch.bias.detach().numpy()\n",
    "    inputs = none\n",
    "    outputLayer = layer.forward(inputs)\n",
    "\n",
    "    # Tra i due modelli ci possono essere differenze di approssimazione dei valori decimali\n",
    "    diff = outputLayer-outputTorch.detach().numpy()\n",
    "    if np.all(np.abs(diff) < 0.00000000001):\n",
    "        print(\"Test passato\")\n",
    "    else:\n",
    "        print(\"Test fallito\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_layer(1000, 2000, nn.ReLU(), ReLuFunction())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_layer(1000, 2000, nn.Sigmoid(), SigmoidFunction())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_layer(1000, 2000, nn.LeakyReLU(), LeakReLuFunction())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_layer(1000, 2000, nn.Tanh(), TanhFunction())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_layer(1000, 2000, nn.Softplus(), SoftplusFunction())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "test_layer(1000, 2000, nn.Identity(), IdentityFunction())"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
